{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstepan-v-kuznetsov\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/esm-2-contact-map/ipynb/wandb/run-20250624_182509-lq014yd2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stepan-v-kuznetsov/deep-origin-task/runs/lq014yd2' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/stepan-v-kuznetsov/deep-origin-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stepan-v-kuznetsov/deep-origin-task' target=\"_blank\">https://wandb.ai/stepan-v-kuznetsov/deep-origin-task</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stepan-v-kuznetsov/deep-origin-task/runs/lq014yd2' target=\"_blank\">https://wandb.ai/stepan-v-kuznetsov/deep-origin-task/runs/lq014yd2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"deep-origin-task\", name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "run.log({'loss': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25012 entries, 0 to 25011\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               25012 non-null  object \n",
      " 1   pdb_id           25012 non-null  object \n",
      " 2   chain_id         25012 non-null  object \n",
      " 3   length           25012 non-null  int64  \n",
      " 4   sequence         25012 non-null  object \n",
      " 5   residue_indices  25012 non-null  object \n",
      " 6   b_factors        25012 non-null  object \n",
      " 7   mean_b_factor    25012 non-null  float64\n",
      " 8   coords_ca        25012 non-null  object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "seqs = pd.read_parquet('../pipeline/output/train_data/train_filtered.parquet')\n",
    "\n",
    "seqs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                      train_106M_A\n",
       "pdb_id                                                          106M\n",
       "chain_id                                                           A\n",
       "length                                                           154\n",
       "sequence           MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...\n",
       "residue_indices    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "b_factors          [32.86, 27.2, 20.55, 18.05, 19.76, 15.24, 11.0...\n",
       "mean_b_factor                                              16.246299\n",
       "coords_ca          [24.708999633789062, 9.586000442504883, -9.829...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_attentions shape: torch.Size([2, 2, 3, 2, 2])\n",
      "tensor([[[[[-1, -1],\n",
      "           [-1, -1]],\n",
      "\n",
      "          [[-2, -2],\n",
      "           [-2, -2]],\n",
      "\n",
      "          [[-3, -3],\n",
      "           [-3, -3]]],\n",
      "\n",
      "\n",
      "         [[[ 1,  1],\n",
      "           [ 1,  1]],\n",
      "\n",
      "          [[ 2,  2],\n",
      "           [ 2,  2]],\n",
      "\n",
      "          [[ 3,  3],\n",
      "           [ 3,  3]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-4, -4],\n",
      "           [-4, -4]],\n",
      "\n",
      "          [[-5, -5],\n",
      "           [-5, -5]],\n",
      "\n",
      "          [[-6, -6],\n",
      "           [-6, -6]]],\n",
      "\n",
      "\n",
      "         [[[ 4,  4],\n",
      "           [ 4,  4]],\n",
      "\n",
      "          [[ 5,  5],\n",
      "           [ 5,  5]],\n",
      "\n",
      "          [[ 6,  6],\n",
      "           [ 6,  6]]]]])\n",
      "Batch 0 before view\n",
      "tensor([[[[-1, -1],\n",
      "          [-1, -1]],\n",
      "\n",
      "         [[-2, -2],\n",
      "          [-2, -2]],\n",
      "\n",
      "         [[-3, -3],\n",
      "          [-3, -3]]],\n",
      "\n",
      "\n",
      "        [[[-4, -4],\n",
      "          [-4, -4]],\n",
      "\n",
      "         [[-5, -5],\n",
      "          [-5, -5]],\n",
      "\n",
      "         [[-6, -6],\n",
      "          [-6, -6]]]])\n",
      "Batch 1 before view\n",
      "tensor([[[[1, 1],\n",
      "          [1, 1]],\n",
      "\n",
      "         [[2, 2],\n",
      "          [2, 2]],\n",
      "\n",
      "         [[3, 3],\n",
      "          [3, 3]]],\n",
      "\n",
      "\n",
      "        [[[4, 4],\n",
      "          [4, 4]],\n",
      "\n",
      "         [[5, 5],\n",
      "          [5, 5]],\n",
      "\n",
      "         [[6, 6],\n",
      "          [6, 6]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m num_layers, batch_size, num_heads, seq_len, _ = all_attentions.shape\n\u001b[32m     33\u001b[39m combined = all_attentions.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m combined = \u001b[43mcombined\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: after view, shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined[:,\u001b[38;5;250m \u001b[39mbatch,\u001b[38;5;250m \u001b[39m:,\u001b[38;5;250m \u001b[39m:].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "num_layers = 2\n",
    "num_heads = 3\n",
    "seq_len = 2\n",
    "\n",
    "# Создаём список attention maps, по одному для каждого слоя\n",
    "attentions = []\n",
    "for layer in range(num_layers):\n",
    "    layer_heads = []\n",
    "    for head in range(num_heads):\n",
    "        value = layer*num_heads + head+1  # уникальное значение для каждого слоя/головы\n",
    "        # attention shape: (batch_size, seq_len, seq_len)\n",
    "        att = torch.full((seq_len, seq_len), value)\n",
    "        layer_heads.append(att)\n",
    "    # Собираем в один слой: (num_heads, seq_len, seq_len)\n",
    "    layer_tensor = torch.stack(layer_heads, dim=0)\n",
    "    batch_tensor = torch.stack([layer_tensor*(-1), layer_tensor], dim=0)\n",
    "    attentions.append(batch_tensor)\n",
    "\n",
    "# Stack по слоям: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "all_attentions = torch.stack(attentions)\n",
    "print(\"all_attentions shape:\", all_attentions.shape)\n",
    "print(all_attentions)\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    print(f\"Batch {batch} before view\")\n",
    "    print(all_attentions[:, batch, :, :, :] )\n",
    "\n",
    "# Переформатируем:\n",
    "num_layers, batch_size, num_heads, seq_len, _ = all_attentions.shape\n",
    "combined = all_attentions.permute(1, 0, 2, 3, 4)\n",
    "combined = combined.view(batch_size, num_layers*num_heads, seq_len, seq_len)\n",
    "for batch in range(batch_size):\n",
    "    print(f\"Batch {batch}: after view, shape {combined[:, batch, :, :].shape}\")\n",
    "    print(combined[:, batch, :, :] )\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    print(f\"Batch {batch} after permute\")\n",
    "    print(combined[batch, :, :, :] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "num_layers = 2\n",
    "num_heads = 3\n",
    "seq_len = 2\n",
    "\n",
    "# Создаём список attention maps, по одному для каждого слоя\n",
    "attentions = []\n",
    "for layer in range(num_layers):\n",
    "    layer_heads = []\n",
    "    for head in range(num_heads):\n",
    "        value = layer*num_heads + head+1  # уникальное значение для каждого слоя/головы\n",
    "        # attention shape: (batch_size, seq_len, seq_len)\n",
    "        att = torch.full((seq_len, seq_len), value)\n",
    "        layer_heads.append(att)\n",
    "\n",
    "    layer_tensor = torch.stack(layer_heads, dim=0)\n",
    "    batch_tensor = torch.stack([layer_tensor*(-1), layer_tensor], dim=0)\n",
    "    attentions.append(batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]],\n",
       "\n",
       "\n",
       "        [[[ 200.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[ 300.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]],\n",
       "\n",
       "\n",
       "        [[[ 400.,  401.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [ 410.,  411.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[ 500.,  501.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [ 510.,  511.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1400., 1401., 1402.,  ...,    0.,    0.,    0.],\n",
       "          [1410., 1411., 1412.,  ...,    0.,    0.,    0.],\n",
       "          [1420., 1421., 1422.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[1500., 1501., 1502.,  ...,    0.,    0.,    0.],\n",
       "          [1510., 1511., 1512.,  ...,    0.,    0.,    0.],\n",
       "          [1520., 1521., 1522.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]],\n",
       "\n",
       "\n",
       "        [[[1600., 1601., 1602.,  ..., 1607.,    0.,    0.],\n",
       "          [1610., 1611., 1612.,  ..., 1617.,    0.,    0.],\n",
       "          [1620., 1621., 1622.,  ..., 1627.,    0.,    0.],\n",
       "          ...,\n",
       "          [1670., 1671., 1672.,  ..., 1677.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[1700., 1701., 1702.,  ..., 1707.,    0.,    0.],\n",
       "          [1710., 1711., 1712.,  ..., 1717.,    0.,    0.],\n",
       "          [1720., 1721., 1722.,  ..., 1727.,    0.,    0.],\n",
       "          ...,\n",
       "          [1770., 1771., 1772.,  ..., 1777.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]],\n",
       "\n",
       "\n",
       "        [[[1800., 1801., 1802.,  ..., 1807., 1808.,    0.],\n",
       "          [1810., 1811., 1812.,  ..., 1817., 1818.,    0.],\n",
       "          [1820., 1821., 1822.,  ..., 1827., 1828.,    0.],\n",
       "          ...,\n",
       "          [1870., 1871., 1872.,  ..., 1877., 1878.,    0.],\n",
       "          [1880., 1881., 1882.,  ..., 1887., 1888.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[1900., 1901., 1902.,  ..., 1907., 1908.,    0.],\n",
       "          [1910., 1911., 1912.,  ..., 1917., 1918.,    0.],\n",
       "          [1920., 1921., 1922.,  ..., 1927., 1928.,    0.],\n",
       "          ...,\n",
       "          [1970., 1971., 1972.,  ..., 1977., 1978.,    0.],\n",
       "          [1980., 1981., 1982.,  ..., 1987., 1988.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 10\n",
    "attentions = torch.arange(10*2*max_len*max_len).reshape(10,2,max_len,max_len)\n",
    "masks = torch.zeros((10, max_len))\n",
    "for i in range(10):\n",
    "    masks[i,:i] = 1\n",
    "\n",
    "mask_2d = masks.unsqueeze(1).unsqueeze(-1) * masks.unsqueeze(1).unsqueeze(-2)\n",
    "\n",
    "attentions * mask_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mask_2d = masks.unsqueeze(1).unsqueeze(-1) * masks.unsqueeze(1).unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "masked_attentions = attentions*mask_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 10, 10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "        [[300.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "        [[500., 501.,   0.,   0.,   0.],\n",
       "         [510., 511.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "        [[700., 701., 702.,   0.,   0.],\n",
       "         [710., 711., 712.,   0.,   0.],\n",
       "         [720., 721., 722.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "        [[900., 901., 902., 903.,   0.],\n",
       "         [910., 911., 912., 913.,   0.],\n",
       "         [920., 921., 922., 923.,   0.],\n",
       "         [930., 931., 932., 933.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attentions[:5,1,:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "symmetric_attentions = (\n",
    "            masked_attentions + masked_attentions.transpose(-1, -2)\n",
    "        ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[300.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[500.0000, 505.5000,   0.0000,   0.0000,   0.0000],\n",
       "         [505.5000, 511.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[700.0000, 705.5000, 711.0000,   0.0000,   0.0000],\n",
       "         [705.5000, 711.0000, 716.5000,   0.0000,   0.0000],\n",
       "         [711.0000, 716.5000, 722.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
       "\n",
       "        [[900.0000, 905.5000, 911.0000, 916.5000,   0.0000],\n",
       "         [905.5000, 911.0000, 916.5000, 922.0000,   0.0000],\n",
       "         [911.0000, 916.5000, 922.0000, 927.5000,   0.0000],\n",
       "         [916.5000, 922.0000, 927.5000, 933.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symmetric_attentions[:5,1,:5,:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 - 300*300/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(505.5+505.5+500+511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014960435212685752"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "505.5 - ((1005.5*1016.5) / (505.5+505.5+500+511))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def apply_apc_correction( attention_maps):\n",
    "    \"\"\"\n",
    "    Apply Average Product Correction (APC) tensorized version.\n",
    "    F^APC_ij = F_ij - (F_i * F_j) / F\n",
    "    \"\"\"\n",
    "    # attention_maps: (batch_size, num_maps, seq_len, seq_len)\n",
    "\n",
    "    # Compute row and column sums\n",
    "    F_i = attention_maps.sum(dim=-1, keepdim=True)  # (batch, num_maps, seq_len, 1)\n",
    "    F_j = attention_maps.sum(dim=-2, keepdim=True)  # (batch, num_maps, 1, seq_len)\n",
    "    F_total = attention_maps.sum(\n",
    "        dim=(-2, -1), keepdim=True\n",
    "    )  # (batch, num_maps, 1, 1)\n",
    "\n",
    "    # Apply APC correction\n",
    "    F_apc = attention_maps - (F_i * F_j) / (F_total + 1e-8)\n",
    "\n",
    "    return F_apc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "F_i = symmetric_attentions.sum(dim=-1, keepdim=True)  # (batch, num_maps, seq_len, 1)\n",
    "F_j = symmetric_attentions.sum(dim=-2, keepdim=True)  # (batch, num_maps, 1, seq_len)\n",
    "F_total = symmetric_attentions.sum(\n",
    "    dim=(-2, -1), keepdim=True\n",
    ")  # (batch, num_maps, 1, 1)\n",
    "\n",
    "# Apply APC correction\n",
    "F_apc = symmetric_attentions - (F_i * F_j) / (F_total + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0.]],\n",
       "\n",
       "        [[  300.]],\n",
       "\n",
       "        [[ 2022.]],\n",
       "\n",
       "        [[ 6399.]],\n",
       "\n",
       "        [[14664.]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_total[:5,1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   0.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000]],\n",
       "\n",
       "        [[ 300.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000]],\n",
       "\n",
       "        [[1005.5000],\n",
       "         [1016.5000],\n",
       "         [   0.0000],\n",
       "         [   0.0000],\n",
       "         [   0.0000]],\n",
       "\n",
       "        [[2116.5000],\n",
       "         [2133.0000],\n",
       "         [2149.5000],\n",
       "         [   0.0000],\n",
       "         [   0.0000]],\n",
       "\n",
       "        [[3633.0000],\n",
       "         [3655.0000],\n",
       "         [3677.0000],\n",
       "         [3699.0000],\n",
       "         [   0.0000]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_i[:5, 1, :5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   0.0000,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
       "\n",
       "        [[ 300.0000,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
       "\n",
       "        [[1005.5000, 1016.5000,    0.0000,    0.0000,    0.0000]],\n",
       "\n",
       "        [[2116.5000, 2133.0000, 2149.5000,    0.0000,    0.0000]],\n",
       "\n",
       "        [[3633.0000, 3655.0000, 3677.0000, 3699.0000,    0.0000]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_j[:5, 1, : ,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1022090.75"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1005.5*1016.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1033272.25"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1016.5*1016.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000]],\n",
       "\n",
       "        [[   90000.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000]],\n",
       "\n",
       "        [[ 1011030.2500,  1022090.7500,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [ 1022090.7500,  1033272.2500,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000]],\n",
       "\n",
       "        [[ 4479572.0000,  4514494.5000,  4549417.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [ 4514494.5000,  4549689.0000,  4584883.5000,        0.0000,\n",
       "                 0.0000],\n",
       "         [ 4549417.0000,  4584883.5000,  4620350.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000]],\n",
       "\n",
       "        [[13198689.0000, 13278615.0000, 13358541.0000, 13438467.0000,\n",
       "                 0.0000],\n",
       "         [13278615.0000, 13359025.0000, 13439435.0000, 13519845.0000,\n",
       "                 0.0000],\n",
       "         [13358541.0000, 13439435.0000, 13520329.0000, 13601223.0000,\n",
       "                 0.0000],\n",
       "         [13438467.0000, 13519845.0000, 13601223.0000, 13682601.0000,\n",
       "                 0.0000],\n",
       "         [       0.0000,        0.0000,        0.0000,        0.0000,\n",
       "                 0.0000]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F_i * F_j)[:5, 1, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "apc_corrected = apply_apc_correction(symmetric_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]]])\n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [10, 11, 12, 13, 14]],\n",
      "\n",
      "        [[ 5,  6,  7,  8,  9],\n",
      "         [15, 16, 17, 18, 19]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(20)\n",
    "print(a)\n",
    "\n",
    "b = a.reshape(2,2, 5)\n",
    "print(b)\n",
    "\n",
    "c = b.transpose(0, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.arange(20).reshape(2,-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.permute(2,1,0)[:,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "print(a)\n",
    "a[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.arange(1000,dtype=torch.float32)\n",
    "a = a.reshape(-1, 10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_i = torch.arange(10).unsqueeze(1)\n",
    "pos_j = torch.arange(10).unsqueeze(0)\n",
    "separation = (pos_i - pos_j) >= 6\n",
    "separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "res = (a*separation).view(a.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82],\n",
       "        [93, 92, 91, 90, 82]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(res, dim=-1, stable=True, descending=True)[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
       "         [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.],\n",
       "         [ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.],\n",
       "         [ 30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
       "         [ 40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.],\n",
       "         [ 50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.],\n",
       "         [ 60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.],\n",
       "         [ 70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.],\n",
       "         [ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.],\n",
       "         [ 90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.]],\n",
       "\n",
       "        [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n",
       "         [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.],\n",
       "         [120., 121., 122., 123., 124., 125., 126., 127., 128., 129.],\n",
       "         [130., 131., 132., 133., 134., 135., 136., 137., 138., 139.],\n",
       "         [140., 141., 142., 143., 144., 145., 146., 147., 148., 149.],\n",
       "         [150., 151., 152., 153., 154., 155., 156., 157., 158., 159.],\n",
       "         [160., 161., 162., 163., 164., 165., 166., 167., 168., 169.],\n",
       "         [170., 171., 172., 173., 174., 175., 176., 177., 178., 179.],\n",
       "         [180., 181., 182., 183., 184., 185., 186., 187., 188., 189.],\n",
       "         [190., 191., 192., 193., 194., 195., 196., 197., 198., 199.]],\n",
       "\n",
       "        [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n",
       "         [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.],\n",
       "         [220., 221., 222., 223., 224., 225., 226., 227., 228., 229.],\n",
       "         [230., 231., 232., 233., 234., 235., 236., 237., 238., 239.],\n",
       "         [240., 241., 242., 243., 244., 245., 246., 247., 248., 249.],\n",
       "         [250., 251., 252., 253., 254., 255., 256., 257., 258., 259.],\n",
       "         [260., 261., 262., 263., 264., 265., 266., 267., 268., 269.],\n",
       "         [270., 271., 272., 273., 274., 275., 276., 277., 278., 279.],\n",
       "         [280., 281., 282., 283., 284., 285., 286., 287., 288., 289.],\n",
       "         [290., 291., 292., 293., 294., 295., 296., 297., 298., 299.]],\n",
       "\n",
       "        [[300., 301., 302., 303., 304., 305., 306., 307., 308., 309.],\n",
       "         [310., 311., 312., 313., 314., 315., 316., 317., 318., 319.],\n",
       "         [320., 321., 322., 323., 324., 325., 326., 327., 328., 329.],\n",
       "         [330., 331., 332., 333., 334., 335., 336., 337., 338., 339.],\n",
       "         [340., 341., 342., 343., 344., 345., 346., 347., 348., 349.],\n",
       "         [350., 351., 352., 353., 354., 355., 356., 357., 358., 359.],\n",
       "         [360., 361., 362., 363., 364., 365., 366., 367., 368., 369.],\n",
       "         [370., 371., 372., 373., 374., 375., 376., 377., 378., 379.],\n",
       "         [380., 381., 382., 383., 384., 385., 386., 387., 388., 389.],\n",
       "         [390., 391., 392., 393., 394., 395., 396., 397., 398., 399.]],\n",
       "\n",
       "        [[400., 401., 402., 403., 404., 405., 406., 407., 408., 409.],\n",
       "         [410., 411., 412., 413., 414., 415., 416., 417., 418., 419.],\n",
       "         [420., 421., 422., 423., 424., 425., 426., 427., 428., 429.],\n",
       "         [430., 431., 432., 433., 434., 435., 436., 437., 438., 439.],\n",
       "         [440., 441., 442., 443., 444., 445., 446., 447., 448., 449.],\n",
       "         [450., 451., 452., 453., 454., 455., 456., 457., 458., 459.],\n",
       "         [460., 461., 462., 463., 464., 465., 466., 467., 468., 469.],\n",
       "         [470., 471., 472., 473., 474., 475., 476., 477., 478., 479.],\n",
       "         [480., 481., 482., 483., 484., 485., 486., 487., 488., 489.],\n",
       "         [490., 491., 492., 493., 494., 495., 496., 497., 498., 499.]],\n",
       "\n",
       "        [[500., 501., 502., 503., 504., 505., 506., 507., 508., 509.],\n",
       "         [510., 511., 512., 513., 514., 515., 516., 517., 518., 519.],\n",
       "         [520., 521., 522., 523., 524., 525., 526., 527., 528., 529.],\n",
       "         [530., 531., 532., 533., 534., 535., 536., 537., 538., 539.],\n",
       "         [540., 541., 542., 543., 544., 545., 546., 547., 548., 549.],\n",
       "         [550., 551., 552., 553., 554., 555., 556., 557., 558., 559.],\n",
       "         [560., 561., 562., 563., 564., 565., 566., 567., 568., 569.],\n",
       "         [570., 571., 572., 573., 574., 575., 576., 577., 578., 579.],\n",
       "         [580., 581., 582., 583., 584., 585., 586., 587., 588., 589.],\n",
       "         [590., 591., 592., 593., 594., 595., 596., 597., 598., 599.]],\n",
       "\n",
       "        [[600., 601., 602., 603., 604., 605., 606., 607., 608., 609.],\n",
       "         [610., 611., 612., 613., 614., 615., 616., 617., 618., 619.],\n",
       "         [620., 621., 622., 623., 624., 625., 626., 627., 628., 629.],\n",
       "         [630., 631., 632., 633., 634., 635., 636., 637., 638., 639.],\n",
       "         [640., 641., 642., 643., 644., 645., 646., 647., 648., 649.],\n",
       "         [650., 651., 652., 653., 654., 655., 656., 657., 658., 659.],\n",
       "         [660., 661., 662., 663., 664., 665., 666., 667., 668., 669.],\n",
       "         [670., 671., 672., 673., 674., 675., 676., 677., 678., 679.],\n",
       "         [680., 681., 682., 683., 684., 685., 686., 687., 688., 689.],\n",
       "         [690., 691., 692., 693., 694., 695., 696., 697., 698., 699.]],\n",
       "\n",
       "        [[700., 701., 702., 703., 704., 705., 706., 707., 708., 709.],\n",
       "         [710., 711., 712., 713., 714., 715., 716., 717., 718., 719.],\n",
       "         [720., 721., 722., 723., 724., 725., 726., 727., 728., 729.],\n",
       "         [730., 731., 732., 733., 734., 735., 736., 737., 738., 739.],\n",
       "         [740., 741., 742., 743., 744., 745., 746., 747., 748., 749.],\n",
       "         [750., 751., 752., 753., 754., 755., 756., 757., 758., 759.],\n",
       "         [760., 761., 762., 763., 764., 765., 766., 767., 768., 769.],\n",
       "         [770., 771., 772., 773., 774., 775., 776., 777., 778., 779.],\n",
       "         [780., 781., 782., 783., 784., 785., 786., 787., 788., 789.],\n",
       "         [790., 791., 792., 793., 794., 795., 796., 797., 798., 799.]],\n",
       "\n",
       "        [[800., 801., 802., 803., 804., 805., 806., 807., 808., 809.],\n",
       "         [810., 811., 812., 813., 814., 815., 816., 817., 818., 819.],\n",
       "         [820., 821., 822., 823., 824., 825., 826., 827., 828., 829.],\n",
       "         [830., 831., 832., 833., 834., 835., 836., 837., 838., 839.],\n",
       "         [840., 841., 842., 843., 844., 845., 846., 847., 848., 849.],\n",
       "         [850., 851., 852., 853., 854., 855., 856., 857., 858., 859.],\n",
       "         [860., 861., 862., 863., 864., 865., 866., 867., 868., 869.],\n",
       "         [870., 871., 872., 873., 874., 875., 876., 877., 878., 879.],\n",
       "         [880., 881., 882., 883., 884., 885., 886., 887., 888., 889.],\n",
       "         [890., 891., 892., 893., 894., 895., 896., 897., 898., 899.]],\n",
       "\n",
       "        [[900., 901., 902., 903., 904., 905., 906., 907., 908., 909.],\n",
       "         [910., 911., 912., 913., 914., 915., 916., 917., 918., 919.],\n",
       "         [920., 921., 922., 923., 924., 925., 926., 927., 928., 929.],\n",
       "         [930., 931., 932., 933., 934., 935., 936., 937., 938., 939.],\n",
       "         [940., 941., 942., 943., 944., 945., 946., 947., 948., 949.],\n",
       "         [950., 951., 952., 953., 954., 955., 956., 957., 958., 959.],\n",
       "         [960., 961., 962., 963., 964., 965., 966., 967., 968., 969.],\n",
       "         [970., 971., 972., 973., 974., 975., 976., 977., 978., 979.],\n",
       "         [980., 981., 982., 983., 984., 985., 986., 987., 988., 989.],\n",
       "         [990., 991., 992., 993., 994., 995., 996., 997., 998., 999.]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1])\n",
      "Precision@3: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def precision_cutoff(true_labels, predictions, k):\n",
    "    sorted_indices = torch.argsort(predictions, stable=True, descending=True)\n",
    "    sorted_indices = sorted_indices[:k]  # Fixed typo in variable name \n",
    "    sorted_true_labels = true_labels[sorted_indices]\n",
    "    print(sorted_true_labels)\n",
    "    return sorted_true_labels.sum() / k\n",
    "\n",
    "# Test the function\n",
    "true_labels = torch.tensor([0, 0, 1, 0, 1])\n",
    "predictions = torch.tensor([0.9, 0.1, 0.8, 0.3, 0.7])\n",
    "k = 3\n",
    "\n",
    "precision = precision_cutoff(true_labels, predictions, k)\n",
    "print(f\"Precision@{k}: {precision}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightning as L\n",
    "\n",
    "logger = WandbLogger(\n",
    "    project=\"deep-origin-task\",\n",
    "    name=\"test\",\n",
    "    log_model=False,\n",
    "    config = {'test': 'test'}\n",
    ")\n",
    "logger.experiment.config.update({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(logger=[logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "logger.log_metrics({\"loss\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
