{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../pipeline/src')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datamodules.paired_dataset import PairedDataset\n",
    "from datamodules.paired_datamodule import PairedProteinDataModule, PairedProteinDataModuleConfig\n",
    "from datamodules.datamodule import ProteinDataModuleConfig, SamplingConfig, ProteinDataModule\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paired dataset from DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset_path = '../pipeline/output/datasets/train.pt'\n",
    "val_dataset_path = '../pipeline/output/datasets/val.pt'\n",
    "test_dataset_path = '../pipeline/output/datasets/test.pt'\n",
    "similarity_file_path = '../pipeline/output/mmseqs/all_seqs_similarity.tsv'\n",
    "cluster_path = '../pipeline/output/mmseqs/all_seqs_clust.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datamodules.paired_datamodule:Num train in queries: 16015\n",
      "INFO:datamodules.paired_datamodule:Num test in queries: 586\n",
      "INFO:datamodules.paired_datamodule:Loaded 16601 similarity pairs: \n",
      "INFO:datamodules.paired_datamodule:                 target_id  identity\n",
      "query_id                            \n",
      "train_4MYS_A  train_1R3D_A     0.387\n",
      "train_4F1K_A  train_4HQF_A     0.928\n",
      "train_6PK0_C  train_6W5G_B     0.426\n",
      "train_6K6W_C  train_7WNT_A     0.389\n",
      "train_3RQ0_A  train_4W65_A     0.924\n",
      "train_150L_A  train_3HUA_A     0.949\n",
      "train_8IQB_A  train_8IQD_B     0.927\n",
      "train_7CI5_A  train_3UHM_A     0.940\n",
      "train_2O5M_X  train_1A6K_A     0.884\n",
      "train_2QB5_B  train_2Q7D_B     0.931\n",
      "INFO:datamodules.datamodule:Sample Train: After selecting 1.0 n/frac of pdbs, train dataset size: 24245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id pdb_id chain_id  length  \\\n",
      "0  train_106M_A   106M        A     154   \n",
      "1  train_109L_A   109L        A     162   \n",
      "2  train_111M_A   111M        A     154   \n",
      "3  train_115L_A   115L        A     162   \n",
      "4  train_122L_A   122L        A     162   \n",
      "\n",
      "                                     residue_indices  mean_b_factor  \n",
      "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      16.246299  \n",
      "1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...      14.398765  \n",
      "2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      20.670714  \n",
      "3  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...      13.986358  \n",
      "4  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...      14.248951  \n",
      "             id pdb_id chain_id  length  \\\n",
      "0  train_1A77_A   1A77        A     315   \n",
      "1  train_1AQJ_A   1AQJ        A     381   \n",
      "2  train_1AQJ_B   1AQJ        B     383   \n",
      "3  train_1C1K_A   1C1K        A     217   \n",
      "4  train_1C90_A   1C90        A     265   \n",
      "\n",
      "                                     residue_indices  mean_b_factor  \n",
      "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...      18.602159  \n",
      "1  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...      24.964724  \n",
      "2  [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...      26.257363  \n",
      "3  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...      10.455899  \n",
      "4  [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...      16.034491  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/esm-2-contact-map/ipynb/../pipeline/src/datamodules/datamodule.py:63: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_pdb = df.groupby(\"pdb_id\").apply(\n",
      "INFO:datamodules.datamodule:Sample Train: After selecting 1 chains per pdb, train dataset size: 14503\n",
      "INFO:datamodules.datamodule:Sample Train: After sampling, train dataset size: 14503\n",
      "INFO:datamodules.datamodule:Sample Val: After selecting 1.0 n/frac of pdbs, val dataset size: 767\n",
      "/workspace/esm-2-contact-map/ipynb/../pipeline/src/datamodules/datamodule.py:63: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_pdb = df.groupby(\"pdb_id\").apply(\n",
      "INFO:datamodules.datamodule:Sample Val: After sampling, val dataset size: 446\n",
      "INFO:datamodules.paired_dataset:Initializing PairedDataset with 14503 sequences and 24245 similar sequences\n",
      "INFO:datamodules.paired_dataset:Filtered similar pairs map to 9275 pairs\n",
      "INFO:datamodules.paired_dataset:Filtered similar dataset to 5903 sequences\n",
      "INFO:datamodules.paired_dataset:Dataset has 14503 sequences and 9275 of them have similar sequence.\n",
      "INFO:datamodules.paired_dataset:Initializing PairedDataset with 446 sequences and 24245 similar sequences\n",
      "INFO:datamodules.paired_dataset:Filtered similar pairs map to 238 pairs\n",
      "INFO:datamodules.paired_dataset:Filtered similar dataset to 235 sequences\n",
      "INFO:datamodules.paired_dataset:Dataset has 446 sequences and 238 of them have similar sequence.\n",
      "INFO:datamodules.paired_datamodule:Num train in queries: 16015\n",
      "INFO:datamodules.paired_datamodule:Num test in queries: 586\n",
      "INFO:datamodules.paired_datamodule:Loaded 16601 similarity pairs: \n",
      "INFO:datamodules.paired_datamodule:                 target_id  identity\n",
      "query_id                            \n",
      "train_6PY3_A  train_6PYI_A     0.929\n",
      "train_3NLL_A  train_5WID_B     0.323\n",
      "train_7BJ5_C  train_7EHS_A     0.363\n",
      "train_7EKD_A  train_6LSV_A     0.327\n",
      "train_5JOX_B  train_5JOZ_B     0.443\n",
      "train_3LJC_A  train_3M65_A     0.421\n",
      "train_7OPV_B  train_7ON7_A     0.885\n",
      "train_6K58_B  train_6LY4_A     0.940\n",
      "train_1UZ4_A  train_4LYR_A     0.359\n",
      "train_4ZBF_I  train_4OQ5_A     0.944\n",
      "INFO:datamodules.datamodule:Sample Test: After selecting 1.0 n/frac of pdbs, test dataset size: 867\n",
      "/workspace/esm-2-contact-map/ipynb/../pipeline/src/datamodules/datamodule.py:63: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_pdb = df.groupby(\"pdb_id\").apply(\n",
      "INFO:datamodules.datamodule:Sample Test: After sampling, test dataset size: 500\n",
      "INFO:datamodules.paired_dataset:Initializing PairedDataset with 500 sequences and 24245 similar sequences\n",
      "INFO:datamodules.paired_dataset:Filtered similar pairs map to 319 pairs\n",
      "INFO:datamodules.paired_dataset:Filtered similar dataset to 307 sequences\n",
      "INFO:datamodules.paired_dataset:Dataset has 500 sequences and 319 of them have similar sequence.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_conf = SamplingConfig(\n",
    "    cluster_path=cluster_path,\n",
    "    train_n_pdb=1.0,\n",
    "    train_intersect_val_clusters=True,\n",
    "    train_intersect_test_clusters=True,\n",
    "    val_n_pdb=1.0,\n",
    "    test_n_pdb=1.0,\n",
    "    train_max_chains_per_pdb=1,\n",
    "    val_max_chains_per_pdb=1,\n",
    "    test_max_chains_per_pdb=1,\n",
    ")\n",
    "paired_datamodule_config = PairedProteinDataModuleConfig(\n",
    "    _target_=\"pipeline.src.datamodules.paired_datamodule.PairedProteinDataModuleConfig\",\n",
    "    name=\"paired_datamodule\",\n",
    "    train_dataset_path=train_dataset_path,\n",
    "    val_dataset_path=val_dataset_path,\n",
    "    test_dataset_path=test_dataset_path,\n",
    "    tokenizer_name=\"facebook/esm2_t33_650M_UR50D\",\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    max_seq_length=1024,\n",
    "    contact_threshold=8.0,\n",
    "    sampler=sampling_conf,\n",
    "    similarity_file_path=similarity_file_path,\n",
    "    min_similarity_threshold=0.3,\n",
    ")\n",
    "\n",
    "paired_datamodule = PairedProteinDataModule(paired_datamodule_config)\n",
    "paired_datamodule.setup()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 130.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUR BATCH 0\n",
      "OUR BATCH 1\n",
      "OUR BATCH 2\n",
      "OUR BATCH 3\n",
      "OUR BATCH 4\n",
      "OUR BATCH 5\n",
      "OUR BATCH 6\n",
      "OUR BATCH 7\n",
      "OUR BATCH 8\n",
      "OUR BATCH 9\n",
      "OUR BATCH 10\n",
      "OUR BATCH 11\n"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(paired_datamodule.train_dataloader())):\n",
    "    print(f\"OUR BATCH {i}\")\n",
    "    # print(batch)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../pipeline/conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[\"model=esm2_lora_template_contactconv_650m\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.esm2_lora_template_contactconvhead import ESM2LoRAContactConvTemplate, ESM2LoRAContactConvTemplateConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ESM2LoRAContactConvTemplateConfig(**cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM2LoRAContactConvTemplateConfig(_target_='esm2_lora_contactconv_template',\n",
      "                                  name='esm2 lora with conv refiner and '\n",
      "                                       'template',\n",
      "                                  backbone='facebook/esm2_t33_650M_UR50D',\n",
      "                                  loss=LossConfig(loss_type='bce',\n",
      "                                                  focal_gamma=2.0,\n",
      "                                                  focal_alpha=-1),\n",
      "                                  wo_lora=False,\n",
      "                                  lora_rank=8,\n",
      "                                  lora_alpha=16,\n",
      "                                  lora_dropout=0.1,\n",
      "                                  target_modules=['query', 'key', 'value'],\n",
      "                                  contact_head_dim=4,\n",
      "                                  learning_rate=0.0005,\n",
      "                                  weight_decay=0.0001,\n",
      "                                  backbone_init_from='../data/checkpoints/best_lora_conv_refiner_k7.ckpt',\n",
      "                                  cnn_kernel_size=7,\n",
      "                                  contact_self_attention_heads=4,\n",
      "                                  contact_cross_attention_heads=4,\n",
      "                                  unfreeze_lora=False)\n"
     ]
    }
   ],
   "source": [
    "pprint(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:models.esm2_lora_contacthead:Loading backbone from ../data/checkpoints/best_lora_conv_refiner_k7.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = ESM2LoRAContactConvTemplate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "batches10 = islice(paired_datamodule.train_dataloader(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.validation_step_outputs.clear()\n",
    "for batch in batches10:\n",
    "    model.validation_step(batch,1)\n",
    "# model.on_validation_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 229])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['similar_sequence']['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 227, 227])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['similar_sequence']['contact_maps'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 226, 226])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['primary_sequence']['contact_maps'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm_model.base_model.model.embeddings.word_embeddings.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.0.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.0.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.1.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.1.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.2.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.2.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.3.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.3.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.4.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.4.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.5.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.5.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.6.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.6.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.7.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.7.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.8.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.8.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.9.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.9.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.10.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.10.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.11.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.11.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.12.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.12.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.13.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.13.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.14.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.14.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.15.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.15.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.16.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.16.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.17.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.17.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.18.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.18.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.19.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.19.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.20.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.20.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.21.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.21.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.22.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.22.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.23.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.23.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.24.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.24.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.25.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.25.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.26.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.26.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.27.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.27.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.28.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.28.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.29.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.29.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.30.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.30.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.31.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.31.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.query.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.query.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.query.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.query.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.key.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.key.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.key.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.key.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.value.base_layer.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.value.base_layer.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.value.lora_A.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.self.value.lora_B.default.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.attention.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.intermediate.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.intermediate.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.output.dense.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.output.dense.bias False\n",
      "esm_model.base_model.model.encoder.layer.32.LayerNorm.weight False\n",
      "esm_model.base_model.model.encoder.layer.32.LayerNorm.bias False\n",
      "esm_model.base_model.model.encoder.emb_layer_norm_after.weight False\n",
      "esm_model.base_model.model.encoder.emb_layer_norm_after.bias False\n",
      "esm_model.base_model.model.pooler.dense.weight False\n",
      "esm_model.base_model.model.pooler.dense.bias False\n",
      "esm_model.base_model.model.contact_head.regression.weight False\n",
      "esm_model.base_model.model.contact_head.regression.bias False\n",
      "contact_head.q_proj.weight True\n",
      "contact_head.q_proj.bias True\n",
      "contact_head.k_proj.weight True\n",
      "contact_head.k_proj.bias True\n",
      "contact_head.cnn_refiner.0.weight True\n",
      "contact_head.cnn_refiner.0.bias True\n",
      "contact_head.cnn_refiner.2.weight True\n",
      "contact_head.cnn_refiner.2.bias True\n",
      "contact_head.final_proj.weight True\n",
      "contact_head.final_proj.bias True\n",
      "contact_self_attention_layer.attn.in_proj_weight True\n",
      "contact_self_attention_layer.attn.in_proj_bias True\n",
      "contact_self_attention_layer.attn.out_proj.weight True\n",
      "contact_self_attention_layer.attn.out_proj.bias True\n",
      "contact_self_attention_layer.norm.weight True\n",
      "contact_self_attention_layer.norm.bias True\n",
      "contact_cross_attention_layer.attn.in_proj_weight True\n",
      "contact_cross_attention_layer.attn.in_proj_bias True\n",
      "contact_cross_attention_layer.attn.out_proj.weight True\n",
      "contact_cross_attention_layer.attn.out_proj.bias True\n",
      "contact_cross_attention_layer.norm.weight True\n",
      "contact_cross_attention_layer.norm.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
